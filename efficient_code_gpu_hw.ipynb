{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shuaiy125/735-project/blob/main/efficient_code_gpu_hw.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81834325",
      "metadata": {
        "id": "81834325"
      },
      "source": [
        "# Writing Efficient Code and GPU Computing Homework\n",
        "\n",
        "Please save your solutions as a **PDF** and upload it to Canvas.\n",
        "\n",
        "## Problem 1: Profiling and Vectorization\n",
        "\n",
        "**(a)** Consider the following cProfile output from a data analysis program:\n",
        "\n",
        "```\n",
        "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
        "     5000    8.234    0.002    8.234    0.002 analysis.py:12(compute_distances)\n",
        "     5000    0.089    0.000    0.089    0.000 analysis.py:28(normalize_vector)\n",
        "  2500000    1.456    0.000    1.456    0.000 analysis.py:35(squared_diff)\n",
        "        1    0.002    0.002    9.781    9.781 analysis.py:50(main)\n",
        "```\n",
        "\n",
        "Which function should you optimize first? Explain your reasoning based on the profiling data. What percentage of the total runtime does this function account for?\n",
        "\n",
        "**Answer:**\n",
        "The function to optimize first is `compute_distances`, as it has the highest total time (`tottime`) of 8.234 seconds out of the total runtime of 9.781 seconds. This means it accounts for approximately 84.2% of the total runtime (calculated as (8.234 / 9.781) * 100). Optimizing this function would likely yield the most significant performance improvement for the overall program.\n",
        "\n",
        "**(b)** The following function computes weighted squared differences between two arrays:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "9b6ed3c6",
      "metadata": {
        "id": "9b6ed3c6"
      },
      "outputs": [],
      "source": [
        "def weighted_squared_diff_loop(x, y, w):\n",
        "    \"\"\"Compute sum of weighted squared differences using a loop.\"\"\"\n",
        "    n = len(x)\n",
        "    total = 0.0\n",
        "    for i in range(n):\n",
        "        diff = x[i] - y[i]\n",
        "        total += w[i] * diff * diff\n",
        "    return total"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab089216",
      "metadata": {
        "id": "ab089216"
      },
      "source": [
        "Write a vectorized version of this function using NumPy operations. Your function should produce the same result but without explicit Python loops. For example, `weighted_squared_diff(np.array([1, 2, 3]), np.array([0, 1, 1]), np.array([1, 2, 3]))` should return `15.0`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11960527",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def weighted_squared_diff(x, y, w): return np.sum(w * (x - y)**2)\n",
        "\n",
        "# Example usage:\n",
        "result = weighted_squared_diff(np.array([1, 2, 3]), np.array([0, 1, 1]), np.array([1, 2, 3]))\n",
        "print(result)  # Output: 15"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bcab2788",
      "metadata": {
        "id": "bcab2788"
      },
      "source": [
        "**(c)** Write a function that transforms an array by replacing negative values with zero and scaling all positive values by their mean. For example, given `np.array([-2, 4, -1, 6, 2])`, the positive values are `[4, 6, 2]` with mean `4.0`, so the result should be `np.array([0, 1, 0, 1.5, 0.5])`. Use boolean indexing instead of loops."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e46bdcf5",
      "metadata": {
        "id": "e46bdcf5",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def transform_array(arr): \n",
        "    arr = np.array(arr, dtype=float) \n",
        "    pos_mask = arr > 0\n",
        "    if np.any(pos_mask):\n",
        "        mean_val = arr[pos_mask].mean()\n",
        "        arr[pos_mask] = arr[pos_mask] / mean_val\n",
        "    arr[~pos_mask] = 0 \n",
        "    return arr\n",
        "\n",
        "# Example usage:\n",
        "result = transform_array(np.array([-2, 4, -1, 6, 2]))\n",
        "print(result) # [0, 1, 0, 1.5, 0.5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b201f5e",
      "metadata": {
        "id": "6b201f5e"
      },
      "source": [
        "## Problem 2: Parallelization and JIT Compilation\n",
        "\n",
        "**(a)** The following function computes the mean of a bootstrap sample:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "dc3d71e0",
      "metadata": {
        "id": "dc3d71e0"
      },
      "outputs": [],
      "source": [
        "def compute_bootstrap_mean(args):\n",
        "    \"\"\"Compute mean of a bootstrap sample.\"\"\"\n",
        "    data, seed = args\n",
        "    rng = np.random.RandomState(seed)\n",
        "    sample = rng.choice(data, size=len(data), replace=True)\n",
        "    return np.mean(sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f42e9db7",
      "metadata": {
        "id": "f42e9db7"
      },
      "source": [
        "Write a function that uses `multiprocessing.Pool` to compute `n_bootstrap` bootstrap means in parallel. Each bootstrap iteration should receive a unique seed to ensure different random samples. Return a list of the bootstrap means."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74f00419",
      "metadata": {
        "id": "74f00419"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import multiprocessing as mp\n",
        "\n",
        "def compute_bootstrap_mean(args):\n",
        "    \"\"\"Compute mean of a bootstrap sample.\"\"\"\n",
        "    data, seed = args\n",
        "    rng = np.random.RandomState(seed)\n",
        "    sample = rng.choice(data, size=len(data), replace=True)\n",
        "    return np.mean(sample)\n",
        "\n",
        "\n",
        "def parallel_bootstrap(data, n_bootstrap, n_workers=4):\n",
        "    \"\"\"Compute bootstrap means in parallel.\"\"\"\n",
        "    seeds = range(n_bootstrap)\n",
        "    tasks = [(data, seed) for seed in seeds]\n",
        "    with mp.Pool(processes=n_workers) as pool:\n",
        "        results = pool.map(compute_bootstrap_mean, tasks)\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "883234d6",
      "metadata": {
        "id": "883234d6"
      },
      "source": [
        "**(b)** Write a Numba-optimized function that computes the running maximum of an array. For each position `i`, the output should contain the maximum of all elements from index 0 to `i` (inclusive). For example, `running_max(np.array([3, 1, 4, 1, 5, 9, 2, 6]))` should return `np.array([3, 3, 4, 4, 5, 9, 9, 9])`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ce3f9fd4",
      "metadata": {
        "id": "ce3f9fd4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1 3 3 5 5]\n"
          ]
        }
      ],
      "source": [
        "from numba import njit\n",
        "import numpy as np\n",
        "\n",
        "@njit\n",
        "def running_max(arr):\n",
        "    \"\"\"Compute running maximum of array.\"\"\"\n",
        "    n = len(arr)\n",
        "    result = np.empty(n, dtype=arr.dtype)\n",
        "    current_max = arr[0]\n",
        "    for i in range(n):\n",
        "        if arr[i] > current_max:\n",
        "            current_max = arr[i]\n",
        "        result[i] = current_max\n",
        "    return result\n",
        "\n",
        "# Example usage:\n",
        "result = running_max(np.array([1, 3, 2, 5, 4]))\n",
        "print(result)  # Output: [1 3 3 5 5]    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba135006",
      "metadata": {
        "id": "ba135006"
      },
      "source": [
        "**(c)** The following Numba function attempts to filter an array to keep only positive values, but it fails to compile. Explain why it fails and provide a corrected version that compiles successfully with `@njit`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83e89816",
      "metadata": {
        "id": "83e89816",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "from numba import njit\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "@njit\n",
        "def filter_positive_broken(arr):\n",
        "    \"\"\"Return array containing only positive values (BROKEN).\"\"\"\n",
        "    result = []\n",
        "    for x in arr:\n",
        "        if x > 0:\n",
        "            result.append(x)\n",
        "    return np.array(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "701aed97",
      "metadata": {},
      "source": [
        "When define result = [], Numba doesn't know what type of data goes inside the list. In compiled machine code, variables must have strict, known types. Also, we generally need to know the size of an array before you write to it to allocate the correct amount of memory. Here's a corrected version that pre-allocates an array for the positive values:\n",
        "\n",
        "```python\n",
        "from numba import njit\n",
        "import numpy as np\n",
        "@njit\n",
        "def filter_positive_fixed(arr):\n",
        "    count = 0\n",
        "    for value in arr:\n",
        "        if value > 0:\n",
        "            count += 1\n",
        "    result = np.empty(count, dtype=arr.dtype)\n",
        "    index = 0\n",
        "    for value in arr:\n",
        "        if value > 0:\n",
        "            result[index] = value\n",
        "            index += 1\n",
        "    return result\n",
        "\n",
        "## Example usage:\n",
        "data = np.array([-5, 3, -1, 4, 0, 2])\n",
        "positive_data = filter_positive_fixed(data)\n",
        "print(positive_data)  # Output: [3 4 2]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94f21d7f",
      "metadata": {
        "id": "94f21d7f"
      },
      "source": [
        "## Problem 3: GPU Computing Fundamentals\n",
        "\n",
        "**(a)** For each of the following computational tasks, state whether it would benefit from GPU acceleration and explain why or why not.\n",
        "\n",
        "1. Computing the mean of 500 numbers\n",
        "2. Multiplying two 5000x5000 matrices\n",
        "3. Reading a 10GB CSV file from disk\n",
        "4. Running 1 million independent Monte Carlo simulations\n",
        "5. Computing Fibonacci numbers recursively\n",
        "\n",
        "**Answer:** \n",
        "1. No, this task is too small to benefit from GPU acceleration due to the overhead of data transfer between CPU and GPU.\n",
        "2. Yes, this task is highly parallelizable and involves a large amount of computation, making it suitable for GPU acceleration.\n",
        "3. No, this is an I/O-bound task rather than a compute-bound task, so GPU acceleration would not help.\n",
        "4. Yes, this task is embarrassingly parallel and can benefit from GPU acceleration as each simulation can be run independently.\n",
        "5. No, this task is inherently sequential and does not benefit from parallelization.\n",
        "\n",
        "**(b)** The following code runs slowly despite using GPU. Identify the performance problem and rewrite the code to fix it. The goal is to compute the sum of squares for 1000 different arrays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bef6c717",
      "metadata": {
        "id": "bef6c717"
      },
      "outputs": [],
      "source": [
        "import cupy as cp\n",
        "import numpy as np\n",
        "\n",
        "results = []\n",
        "for i in range(1000):\n",
        "    data = np.random.randn(10000)  # Generate on CPU\n",
        "    gpu_data = cp.asarray(data)    # Transfer to GPU\n",
        "    result = cp.sum(gpu_data ** 2) # Compute on GPU\n",
        "    results.append(result.get())   # Transfer back to CPU\n",
        "\n",
        "print(f\"Total: {sum(results)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f55b766",
      "metadata": {},
      "source": [
        "**Ans:** The performance problem is excessive data transfer between the CPU and GPU. Inside the loop, the codes move data from Host (CPU) to Device (GPU) and back 1000 times, which is inefficient. Also, by generating random numbers on the CPU, the codes are forcing the high-speed GPU to wait for the slower CPU. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ae78333",
      "metadata": {
        "id": "5ae78333"
      },
      "source": [
        "Write an efficient version that minimizes data transfers between CPU and GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e860d5f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import cupy as cp\n",
        "import numpy as np\n",
        "\n",
        "all_data_gpu = cp.random.randn(1000, 10000)\n",
        "total_sum_gpu = cp.sum(all_data_gpu ** 2)\n",
        "total_result = total_sum_gpu.get()\n",
        "print(f\"Total: {total_result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7265175a",
      "metadata": {
        "id": "7265175a"
      },
      "source": [
        "## Problem 4: CuPy and PyTorch\n",
        "\n",
        "**(a)** Convert the following NumPy code to CuPy. The function computes z-score normalization and then the correlation matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86981b8b",
      "metadata": {
        "id": "86981b8b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def correlation_matrix_numpy(X):\n",
        "    \"\"\"Compute correlation matrix after z-score normalization.\n",
        "\n",
        "    X has shape (n_samples, n_features).\n",
        "    \"\"\"\n",
        "    # Z-score normalize each column\n",
        "    mean = np.mean(X, axis=0)\n",
        "    std = np.std(X, axis=0)\n",
        "    Z = (X - mean) / std\n",
        "\n",
        "    # Compute correlation matrix\n",
        "    n = X.shape[0]\n",
        "    corr = (Z.T @ Z) / n\n",
        "    return corr"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f725bb1e",
      "metadata": {
        "id": "f725bb1e"
      },
      "source": [
        "Write the CuPy version that performs the computation on GPU and returns the result as a NumPy array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cad7c938",
      "metadata": {
        "id": "cad7c938"
      },
      "outputs": [],
      "source": [
        "import cupy as cp\n",
        "import numpy as np\n",
        "\n",
        "def correlation_matrix_cupy(X):\n",
        "    X_gpu = cp.asarray(X)\n",
        "    mean_gpu = cp.mean(X_gpu, axis=0)\n",
        "    std_gpu = cp.std(X_gpu, axis=0)\n",
        "    Z_gpu = (X_gpu - mean_gpu) / std_gpu\n",
        "    n = X_gpu.shape[0]\n",
        "    corr_gpu = (Z_gpu.T @ Z_gpu) / n\n",
        "    return corr_gpu.get()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe047ad6",
      "metadata": {
        "id": "fe047ad6"
      },
      "source": [
        "**(b)** The following PyTorch code has a bug that causes a runtime error. Identify the error and provide the corrected code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fd55abb",
      "metadata": {
        "id": "6fd55abb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def process_data(numpy_array):\n",
        "    \"\"\"Process data using PyTorch on GPU.\"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Convert to tensor and move to GPU\n",
        "    x = torch.from_numpy(numpy_array).to(device)\n",
        "\n",
        "    # Create another tensor for computation\n",
        "    weights = torch.ones(len(numpy_array))\n",
        "\n",
        "    # Weighted sum\n",
        "    result = torch.sum(x * weights)\n",
        "\n",
        "    return result.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bc5f49d",
      "metadata": {},
      "source": [
        "**Ans:** The Error: The code fails because it tries to multiply two tensors located on different devices (CPU and GPU): x is moved to the GPU while weights is created on the CPU by default.\n",
        "\n",
        "Corrected Code:\n",
        "```python\n",
        "import torch\n",
        "def process_data(numpy_array):\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device('cuda')\n",
        "    elif torch.backends.mps.is_available():\n",
        "        device = torch.device('mps') \n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "\n",
        "    x = torch.from_numpy(numpy_array).to(device)\n",
        "    weights = torch.ones(len(numpy_array), device=device)\n",
        "    result = torch.sum(x * weights)\n",
        "\n",
        "    return result.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "870e56d4",
      "metadata": {
        "id": "870e56d4"
      },
      "source": [
        "**(c)** Explain why the following GPU timing code gives incorrect measurements. Then provide corrected code that accurately measures GPU computation time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bcdf793",
      "metadata": {
        "id": "4bcdf793",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import time\n",
        "\n",
        "device = torch.device('cuda')\n",
        "a = torch.randn(5000, 5000, device=device)\n",
        "b = torch.randn(5000, 5000, device=device)\n",
        "\n",
        "start = time.perf_counter()\n",
        "c = torch.mm(a, b)\n",
        "elapsed = time.perf_counter() - start\n",
        "print(f\"Time: {elapsed*1000:.2f} ms\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2cfaac5",
      "metadata": {},
      "source": [
        "**Ans:** The timing code gives incorrect measurements because GPU operations are asynchronous. When you call a GPU operation, it is queued and may not complete immediately. Therefore, the time measured does not account for the actual computation time on the GPU.\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import time\n",
        "\n",
        "device = torch.device('cuda')\n",
        "a = torch.randn(5000, 5000, device=device)\n",
        "b = torch.randn(5000, 5000, device=device)\n",
        "torch.cuda.synchronize()\n",
        "start = time.perf_counter()\n",
        "c = torch.mm(a, b)\n",
        "torch.cuda.synchronize()\n",
        "elapsed = time.perf_counter() - start\n",
        "print(f\"Time: {elapsed*1000:.2f} ms\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb007cff",
      "metadata": {
        "id": "bb007cff"
      },
      "source": [
        "## Problem 5: Performance Comparison\n",
        "\n",
        "**(a)** In extreme value statistics, we often need to estimate the probability that the maximum of n independent standard normal random variables exceeds a threshold t. This can be done via Monte Carlo simulation: generate n normal values, take the maximum, and check if it exceeds t. Repeat this many times and compute the proportion that exceed t.\n",
        "\n",
        "Implement two versions of this simulation:\n",
        "\n",
        "1. A Numba-optimized CPU version using `@njit`\n",
        "2. A CuPy GPU version using vectorized operations\n",
        "\n",
        "Both functions should take parameters `n` (number of normal values per trial), `t` (threshold), and `n_simulations` (number of Monte Carlo trials), and return the estimated probability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "518b5f29",
      "metadata": {
        "id": "518b5f29"
      },
      "outputs": [],
      "source": [
        "from numba import njit\n",
        "import numpy as np\n",
        "import cupy as cp\n",
        "\n",
        "@njit\n",
        "def estimate_prob_numba(n, t, n_simulations):\n",
        "    count = 0\n",
        "    for i in range(n_simulations):\n",
        "        current_max = -np.inf\n",
        "        for j in range(n):\n",
        "            val = np.random.randn()\n",
        "            if val > current_max:\n",
        "                current_max = val\n",
        "        \n",
        "        if current_max > t:\n",
        "            count += 1\n",
        "            \n",
        "    return count / n_simulations\n",
        "\n",
        "def estimate_prob_cupy(n, t, n_simulations):\n",
        "    data = cp.random.randn(n_simulations, n)\n",
        "    max_vals = cp.max(data, axis=1)\n",
        "    probability = cp.mean(max_vals > t)\n",
        "    return probability.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59b134e3",
      "metadata": {
        "id": "59b134e3"
      },
      "source": [
        "**(b)** Design an experiment to find the \"crossover point\" where the GPU version becomes faster than the CPU version. Your experiment should vary the problem size (e.g., `n_simulations`) and measure execution time for both implementations. Describe what factors affect where this crossover occurs and what values you would test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a563554f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import cupy as cp\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def benchmark_crossover():\n",
        "    n = 100\n",
        "    t = 2.0\n",
        "    simulation_counts = [1_000, 10_000, 100_000, 1_000_000, 10_000_000]\n",
        "    results = []\n",
        "\n",
        "    print(f\"{'Simulations':<15} | {'Numba (CPU)':<15} | {'CuPy (GPU)':<15} | {'Winner':<10}\")\n",
        "    print(\"-\" * 65)\n",
        "\n",
        "    estimate_prob_numba(10, t, 10)\n",
        "    estimate_prob_cupy(10, t, 10)\n",
        "\n",
        "    for n_sim in simulation_counts:\n",
        "        start_cpu = time.perf_counter()\n",
        "        estimate_prob_numba(n, t, n_sim)\n",
        "        end_cpu = time.perf_counter()\n",
        "        time_cpu = end_cpu - start_cpu\n",
        "\n",
        "        cp.get_default_memory_pool().free_all_blocks()\n",
        "        cp.cuda.Device().synchronize()\n",
        "        start_gpu = time.perf_counter()\n",
        "        \n",
        "        estimate_prob_cupy(n, t, n_sim)\n",
        "        \n",
        "        cp.cuda.Device().synchronize()\n",
        "        end_gpu = time.perf_counter()\n",
        "        time_gpu = end_gpu - start_gpu\n",
        "\n",
        "        winner = \"CPU\" if time_cpu < time_gpu else \"GPU\"\n",
        "        results.append({\n",
        "            \"n_simulations\": n_sim,\n",
        "            \"CPU_Time\": time_cpu,\n",
        "            \"GPU_Time\": time_gpu,\n",
        "            \"Winner\": winner\n",
        "        })\n",
        "        \n",
        "        print(f\"{n_sim:<15} | {time_cpu:.6f}s | {time_gpu:.6f}s | {winner}\")\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "df_results = benchmark_crossover()\n",
        "print(df_results.to_markdown(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9904f57d",
      "metadata": {},
      "source": [
        "**Factors Affecting the Crossover Point:** \n",
        "\n",
        "1. Kernel Launch Overhead: The time taken to launch GPU kernels can be significant for small problem sizes, making the CPU faster for smaller workloads.\n",
        "2. Core Saturation: A GPU has thousands of cores (e.g., 2,000â€“10,000). To be efficient, it needs enough work to keep all of them busy simultaneously.\n",
        "3. Memory Transfer vs. Compute Ratio: If the system runs out of GPU memory (VRAM), it might crash or slow down. The Numba version uses almost no memory.\n",
        "\n",
        "**Test values:** With the fixed values n = 100, t = 2, run the Test values in Logarithmic scale: [1_000, 10000, 100000, 1000000, 10000000]."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28f32c8c",
      "metadata": {},
      "source": [
        "**(c)** Suppose you need to run a very large simulation with `n_simulations = 100_000_000` but your GPU only has 8GB of memory. The naive CuPy implementation would require generating a matrix of shape `(n_simulations, n)` which may not fit in memory. Write a batched version that processes the simulations in chunks to stay within memory limits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f51590a",
      "metadata": {
        "id": "3f51590a"
      },
      "outputs": [],
      "source": [
        "import cupy as cp\n",
        "\n",
        "def estimate_prob_cupy_batched(n, t, n_simulations, batch_size=1_000_000):\n",
        "    total_successes = 0\n",
        "    for i in range(0, n_simulations, batch_size):\n",
        "        current_batch_size = min(batch_size, n_simulations - i)\n",
        "        batch_data = cp.random.randn(current_batch_size, n)\n",
        "        batch_maxs = cp.max(batch_data, axis=1)\n",
        "        count = cp.sum(batch_maxs > t).item()\n",
        "        total_successes += count\n",
        "    return total_successes / n_simulations"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
